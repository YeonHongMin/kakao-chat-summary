"""
url_extractor.py - URL ì¶”ì¶œ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ìš”ì•½ëœ Markdown í…ìŠ¤íŠ¸ì—ì„œ URLì„ ì¶”ì¶œí•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- "### ğŸ”— ë§í¬/URL" ì„¹ì…˜ì—ì„œ URL ì¶”ì¶œ
- URLê³¼ í•¨ê»˜ ì„¤ëª… í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ì¤‘ë³µ URL ì œê±° (ì„¤ëª…ì€ ë³‘í•©)
- ê²°ê³¼ë¥¼ ë³„ë„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥

ì‚¬ìš©ë²•:
    python url_extractor.py <file_or_directory>
    python url_extractor.py  # data ë””ë ‰í„°ë¦¬ ê¸°ë³¸ ìŠ¤ìº”
"""

import re
from collections import defaultdict
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple

# URL ì¶”ì¶œì„ ìœ„í•œ ì •ê·œí‘œí˜„ì‹ íŒ¨í„´
# http:// ë˜ëŠ” https://ë¡œ ì‹œì‘í•˜ëŠ” URLì„ ë§¤ì¹­
# ê³µë°±, ê´„í˜¸, í•œê¸€ ë“±ì—ì„œ URL ì¢…ë£Œ
URL_PATTERN = re.compile(
    r'(https?://[^\s<>"\')\]ê°€-í£]+)',
    re.IGNORECASE
)


def extract_url_with_description(line: str) -> Tuple[str, str]:
    """
    í•œ ì¤„ì˜ í…ìŠ¤íŠ¸ì—ì„œ URLê³¼ ì„¤ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    ì…ë ¥ ì˜ˆì‹œ:
    - "[ë‹‰ë„¤ì„] https://example.com (ì„¤ëª…)"
    - "https://example.com - ìœ ìš©í•œ ë„êµ¬"
    
    Args:
        line: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ë¼ì¸
        
    Returns:
        (URL, ì„¤ëª…) íŠœí”Œ. URLì´ ì—†ìœ¼ë©´ ("", "") ë°˜í™˜
    """
    # [ë‹‰ë„¤ì„] ì´ë‚˜ [ì‹œê°„] ê°™ì€ ë©”íƒ€ë°ì´í„° ì œê±°
    line_without_sender = re.sub(r'\[.*?\]', '', line).strip()
    
    # ë¦¬ìŠ¤íŠ¸ ë§ˆì»¤ "- " ì œê±°
    if line_without_sender.startswith('- '):
        line_without_sender = line_without_sender[2:].strip()
    
    # URL ê²€ìƒ‰
    url_match = URL_PATTERN.search(line_without_sender)
    if not url_match:
        return "", ""
    
    url = url_match.group(1)
    
    # URL ëì— ë¶™ì€ êµ¬ë‘ì  ì œê±° (ì •ê·œí‘œí˜„ì‹ì´ ê³¼ë„í•˜ê²Œ ë§¤ì¹­í•˜ëŠ” ê²½ìš°)
    while url and url[-1] in '.,;:!?)]\'"':
        url = url[:-1]
    
    # URL ì´í›„ í…ìŠ¤íŠ¸ì—ì„œ ì„¤ëª… ì¶”ì¶œ
    after_url = line_without_sender[url_match.end():].strip()
    
    # ê´„í˜¸ ì•ˆì˜ ë‚´ìš©ì„ ì„¤ëª…ìœ¼ë¡œ ì‚¬ìš© (ì˜ˆ: https://... (ì„¤ëª…))
    paren_match = re.search(r'\((.+)\)', after_url)
    if paren_match:
        description = paren_match.group(1).strip()
    else:
        # ê´„í˜¸ê°€ ì—†ìœ¼ë©´ URL ì•ë’¤ í…ìŠ¤íŠ¸ë¥¼ ì„¤ëª…ìœ¼ë¡œ ì‚¬ìš©
        before_url = line_without_sender[:url_match.start()].strip()
        description = (before_url + " " + after_url).strip()
        
        # ì½œë¡ ìœ¼ë¡œ ì‹œì‘í•˜ë©´ ì œê±°
        if description.startswith(':'):
            description = description[1:].strip()
        
        # ë¹ˆ ê´„í˜¸ ì œê±°
        description = re.sub(r'\(\s*\)', '', description).strip()
    
    return url, description


def extract_urls_from_text(text: str) -> Dict[str, List[str]]:
    """
    í…ìŠ¤íŠ¸ì˜ "ë§í¬/URL" ì„¹ì…˜ì—ì„œ URLê³¼ ì„¤ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    "### ğŸ”— ë§í¬/URL" ë˜ëŠ” ìœ ì‚¬í•œ í—¤ë”ë¡œ ì‹œì‘í•˜ëŠ” ì„¹ì…˜ì„ ì°¾ê³ ,
    í•´ë‹¹ ì„¹ì…˜ ë‚´ì˜ ëª¨ë“  URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        text: ë¶„ì„í•  ì „ì²´ í…ìŠ¤íŠ¸ (Markdown í˜•ì‹)
        
    Returns:
        {URL: [ì„¤ëª… ëª©ë¡]} ë”•ì…”ë„ˆë¦¬
        ê°™ì€ URLì´ ì—¬ëŸ¬ ë²ˆ ë“±ì¥í•˜ë©´ ì„¤ëª…ë“¤ì´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ë¨
    """
    url_descriptions = defaultdict(list)
    in_url_section = False  # í˜„ì¬ URL ì„¹ì…˜ ë‚´ë¶€ì¸ì§€ ì—¬ë¶€
    
    for line in text.split('\n'):
        line = line.strip()
        
        # URL ì„¹ì…˜ ì‹œì‘ ê°ì§€ (ë‹¤ì–‘í•œ í—¤ë” í˜•ì‹ ì§€ì›)
        if '### ë§í¬' in line or '### URL' in line or '2. ê³µìœ ëœ ì¤‘ìš” ë§í¬' in line:
            in_url_section = True
            continue
        
        # ë‹¤ë¥¸ ì„¹ì…˜ ì‹œì‘ ê°ì§€ (URL ì„¹ì…˜ ì¢…ë£Œ)
        # "###", "##", "3." ë“±ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ìƒˆë¡œìš´ í—¤ë”
        if in_url_section and (line.startswith('### ') or line.startswith('## ') or (line[:2].isdigit() and line[2] == '.')):
             # ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ("-")ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì„¹ì…˜ ì¢…ë£Œë¡œ íŒë‹¨
             if not line.startswith('-'): 
                 if line and not line.startswith('http'):
                     in_url_section = False
                     continue
        
        # URL ì„¹ì…˜ ë‚´ì—ì„œ URL ì¶”ì¶œ
        if in_url_section:
            url, description = extract_url_with_description(line)
            if url:
                # ì¤‘ë³µ ì„¤ëª… ë°©ì§€: ê°™ì€ ì„¤ëª…ì€ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                if description and description not in url_descriptions[url]:
                    url_descriptions[url].append(description)
                elif not description and url not in url_descriptions:
                    # ì„¤ëª… ì—†ëŠ” URLë„ ë“±ë¡ (ë¹ˆ ë¦¬ìŠ¤íŠ¸)
                    if not url_descriptions[url]:
                        url_descriptions[url] = []
    
    return dict(url_descriptions)


def save_urls_to_file(url_dict: Dict[str, List[str]], output_path: str, chatroom_name: str = "Unknown") -> None:
    """
    ì¶”ì¶œëœ URL ëª©ë¡ì„ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        url_dict: {URL: [ì„¤ëª… ëª©ë¡]} ë”•ì…”ë„ˆë¦¬
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        chatroom_name: ì±„íŒ…ë°© ì´ë¦„ (í—¤ë”ì— í‘œì‹œ)
    """
    # URLì„ ì•ŒíŒŒë²³ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_urls = sorted(url_dict.items(), key=lambda x: x[0].lower())
    
    with open(output_path, "w", encoding="utf-8") as f:
        # í—¤ë” ì •ë³´ ì‘ì„±
        f.write(f"ğŸ”— [{chatroom_name}] URL ëª©ë¡\n")
        f.write(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ì´ {len(url_dict)}ê°œ URL\n")
        f.write("=" * 60 + "\n\n")
        
        # URLê³¼ ì„¤ëª… ì¶œë ¥
        for url, descriptions in sorted_urls:
            if descriptions:
                # ì—¬ëŸ¬ ì„¤ëª…ì´ ìˆìœ¼ë©´ " / "ë¡œ ì—°ê²°
                merged_desc = " / ".join(descriptions)
                f.write(f"{url} ({merged_desc})\n")
            else:
                f.write(f"{url}\n")


def main():
    """
    ë…ë¦½ ì‹¤í–‰ ì‹œ ë©”ì¸ í•¨ìˆ˜.
    
    ëª…ë ¹ì¤„ ì¸ìë¡œ íŒŒì¼ ë˜ëŠ” ë””ë ‰í„°ë¦¬ ê²½ë¡œë¥¼ ë°›ì•„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ì¸ìê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ data ë””ë ‰í„°ë¦¬ë¥¼ ìŠ¤ìº”í•©ë‹ˆë‹¤.
    """
    import sys
    
    # ëª…ë ¹ì¤„ ì¸ì í™•ì¸
    if len(sys.argv) < 2:
        # ê¸°ë³¸ ê²½ë¡œ: srcì˜ ìƒìœ„ ë””ë ‰í„°ë¦¬ -> data
        base_dir = Path(__file__).resolve().parent.parent
        data_dir = base_dir / 'data'
        print("Usage: python url_extractor.py <file_or_directory>")
        target_path = data_dir
    else:
        target_path = Path(sys.argv[1]).expanduser()
    
    # ê²½ë¡œ ì¡´ì¬ í™•ì¸
    if not target_path.exists():
        print(f"âŒ Path not found: {target_path}")
        sys.exit(1)
        
    # ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼ ëª©ë¡ êµ¬ì„±
    targets = []
    if target_path.is_file():
        targets.append(target_path)
    else:
        # ë””ë ‰í„°ë¦¬ì¸ ê²½ìš°: *_summary.md íŒŒì¼ ê²€ìƒ‰
        targets = list(target_path.glob("*_summary.md"))
        
    if not targets:
        print("âŒ No matching files (*_summary.md) found.")
        return
        
    print(f"ğŸ” Found {len(targets)} files.\n")
    
    # ê° íŒŒì¼ ì²˜ë¦¬
    for file_path in targets:
        print(f"Processing: {file_path.name}")
        try:
            text = file_path.read_text(encoding='utf-8')
            url_dict = extract_urls_from_text(text)
            
            if url_dict:
                # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±: *_summary.md -> *_url.txt
                output_filename = file_path.stem.replace("_summary", "") + "_url.txt"
                if output_filename == file_path.name: 
                     output_filename = file_path.stem + "_url.txt"
                
                output_path = file_path.parent / output_filename
                save_urls_to_file(url_dict, str(output_path), file_path.stem)
                print(f"  âœ… Saved: {output_filename}")
            else:
                print("  â„¹ï¸  No URLs found.")
        except Exception as e:
            print(f"  âŒ Error: {e}")


if __name__ == "__main__":
    main()
